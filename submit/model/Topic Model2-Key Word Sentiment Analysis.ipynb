{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from copy import copy, deepcopy\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from pandas.tseries.offsets import *\n",
    "import re\n",
    "import collections\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_ads = \"/Users/MaxTan/Documents/CU_16fall/BDA/Project/Data/\"\n",
    "\n",
    "user_raw = pd.read_csv(data_ads + \"wow3_user2.csv\", names = [\"user_id\", \"review_count\", \"average_stars\", \n",
    "                                               \"friends\", \"fans\", \"votes_cool\", \"votes_funny\", \n",
    "                                               \"votes_useful\", \"num\", \"degree\", \"coefficient\"])\n",
    "\n",
    "business_raw = pd.read_csv(data_ads+\"wow3_business_mysql.csv\", names = [\"business_id\", \"name\", \"latitude\", \n",
    "                                                           \"longitude\", \"stars\", \"categories\", \n",
    "                                                           \"review_count\", \"open\"])\n",
    "\n",
    "review_raw = pd.read_csv(data_ads+\"wow3_review_mysql.csv\", names = [\"review_id\", \"user_id\", \"business_id\", \n",
    "                                                       \"stars\", \"text\", \"date\", \"votes_funny\", \n",
    "                                                       \"votes_useful\"])\n",
    "\n",
    "all_raw = pd.read_csv(data_ads+\"wow3_all2.csv\", names = [\"business_id\", \n",
    "                                                       \"name\", \"stars_business\",\"business_review_count\",\n",
    "                                                       \"categories\", \"open\", \"review_id\", \n",
    "                                                       \"stars_review\", \"text\", \"date\", \"votes_funny\", \n",
    "                                                       \"votes_useful\",\"user_id\", \"user_review_count\", \n",
    "                                                       \"average_stars\",\"friends\", \"fans\", \"num\", \n",
    "                                                       \"degree\", \"coefficient\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.Topic Model and Sentimental Analysis by Using Key Word:\n",
    "search_word = 'pizza'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbus_gb_df = deepcopy(all_raw)\\n#Group by different businesses for different rates:\\nbus_df = bus_gb_df.groupby([\\'business_id\\']).mean()\\nbus_df[\\'business_id\\'] = bus_df.index\\nbus_df.index = range(len(bus_df))\\nbus = list(set(bus_df[\\'business_id\\']))\\nind = [True if i in bus else False for i in business_raw[\\'business_id\\']]\\nbusiness_revised = business_raw[ind]\\n\\n#Merge two tables:\\nlda_df = pd.merge(bus_df,business_revised,on=\\'business_id\\')\\n\\nstart_time = time.time()\\nbus_topic = dict()\\n\\nfor i in range(len(all_raw)):\\n    ly = all_raw.text[i].lower()\\n    ly = ly.decode(\\'utf-8\\')\\n    \\n    #1.Tokenization:\\n    tokens = nltk.word_tokenize(ly)\\n    #2.Special Characters Removed:\\n    tokens = [re.sub(\\'[^A-Za-z0-9]+\\', \\'\\', mystring) for mystring in tokens]\\n    tokens_lem_stem = []\\n    for word in tokens:\\n        try:\\n            #3.First Lemmatization, 4.then Stemming:\\n            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\\n        except UnicodeDecodeError:\\n            continue\\n\\n    #5.Remove Punctuation:\\n    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\\n    #6.Remove stop words:\\n    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\\n\\n    tokens = word_stop\\n    \\n    \\n    temp_bus = all_raw.business_id[i]\\n    if temp_bus not in bus_topic:\\n        bus_topic[temp_bus] = tokens\\n    else:\\n        bus_topic[temp_bus] += tokens\\n\\n\\n\\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\\n\\nlda_df[\\'tokens\\'] = pd.Series([\\'\\']*len(lda_df),index = lda_df.index)\\nprint len(bus_topic)\\nfor bus in bus_topic:\\n    #print bus\\n    tempdf = lda_df[lda_df[\\'business_id\\']==bus]\\n    temptext = tempdf[\\'categories\\'].values[0].lower()[1:-1]+\\', \\'.join(bus_topic[bus])\\n    lda_df.set_value([tempdf.index[0]],\\'tokens\\',temptext)\\n\\nlda_df.to_csv(\\'business_LDA.csv\\')'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The following codes take 20 minutes to generate a new csv file, which I've store it in the same folder:\n",
    "\"\"\"\n",
    "bus_gb_df = deepcopy(all_raw)\n",
    "#Group by different businesses for different rates:\n",
    "bus_df = bus_gb_df.groupby(['business_id']).mean()\n",
    "bus_df['business_id'] = bus_df.index\n",
    "bus_df.index = range(len(bus_df))\n",
    "bus = list(set(bus_df['business_id']))\n",
    "ind = [True if i in bus else False for i in business_raw['business_id']]\n",
    "business_revised = business_raw[ind]\n",
    "\n",
    "#Merge two tables:\n",
    "lda_df = pd.merge(bus_df,business_revised,on='business_id')\n",
    "\n",
    "start_time = time.time()\n",
    "bus_topic = dict()\n",
    "\n",
    "for i in range(len(all_raw)):\n",
    "    ly = all_raw.text[i].lower()\n",
    "    ly = ly.decode('utf-8')\n",
    "    \n",
    "    #1.Tokenization:\n",
    "    tokens = nltk.word_tokenize(ly)\n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    tokens = word_stop\n",
    "    \n",
    "    \n",
    "    temp_bus = all_raw.business_id[i]\n",
    "    if temp_bus not in bus_topic:\n",
    "        bus_topic[temp_bus] = tokens\n",
    "    else:\n",
    "        bus_topic[temp_bus] += tokens\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "lda_df['tokens'] = pd.Series(['']*len(lda_df),index = lda_df.index)\n",
    "print len(bus_topic)\n",
    "for bus in bus_topic:\n",
    "    #print bus\n",
    "    tempdf = lda_df[lda_df['business_id']==bus]\n",
    "    temptext = tempdf['categories'].values[0].lower()[1:-1]+', '.join(bus_topic[bus])\n",
    "    lda_df.set_value([tempdf.index[0]],'tokens',temptext)\n",
    "\n",
    "lda_df.to_csv('business_LDA.csv')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Tokenizer:\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# Create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# Load CSV file directly:\n",
    "lda_df2 = pd.read_csv('business_LDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#1. Sort by stars:\n",
    "ind = [True if search_word in token else False for token in lda_df2.tokens.values]\n",
    "lda_df_sub = lda_df2[ind]\n",
    "lda_df_target = lda_df_sub.sort(['stars_review'],ascending=[0]).iloc[:6]\n",
    "doc_set = list(lda_df_target.tokens.values)\n",
    "\"\"\"\n",
    "#2. Sort by number of key word:\n",
    "lda_df2['count_sw'] = pd.Series([0]*len(lda_df2),index = lda_df2.index)\n",
    "for i in range(len(lda_df2)):\n",
    "    temp_value = collections.Counter([item.strip() for item in lda_df2.tokens[i].split(\",\")])[search_word]\n",
    "    lda_df2.set_value(i,'count_sw',temp_value)\n",
    "lda_df_target = lda_df2.sort(['count_sw'],ascending=[0]).iloc[:3]\n",
    "doc_set = list(lda_df_target.tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate of training set: 1.0\n",
      "Correct oob rate of training set: 0.847402597403\n",
      "great [1]\n",
      "good [1]\n",
      "wa [1]\n",
      "beer [1]\n",
      "veri [1]\n",
      "nt [1]\n",
      "pizza [1]\n",
      "place [1]\n",
      "thi [1]\n",
      "order [1]\n",
      "--- 394.412029028 seconds ---\n",
      "Correct rate of training set: 1.0\n",
      "Correct oob rate of training set: 1.0\n",
      "wa [1]\n",
      "mineo [1]\n",
      "chee [1]\n",
      "place [1]\n",
      "thi [1]\n",
      "nt [1]\n",
      "pizza [1]\n",
      "good [1]\n",
      "like [1]\n",
      "pittsburgh [1]\n",
      "--- 347.601261854 seconds ---\n",
      "Correct rate of training set: 1.0\n",
      "Correct oob rate of training set: 1.0\n",
      "wa [1]\n",
      "gelato [1]\n",
      "thi [1]\n",
      "flavor [1]\n",
      "nt [1]\n",
      "pizza [1]\n",
      "good [1]\n",
      "like [1]\n",
      "place [1]\n",
      "mercurio [1]\n",
      "--- 379.595090866 seconds ---\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc_set)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    texts = []\n",
    "    # clean and tokenize document string\n",
    "    raw = doc_set[i].lower()\n",
    "    #1.Tokenization:\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, \n",
    "            #4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation] \n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")] \n",
    "    # add tokens to list\n",
    "    texts.append(word_stop)\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=50)\n",
    "    result = ldamodel.print_topics(num_topics=10, num_words=10)\n",
    "    result1 = [j[1] for j in result]\n",
    "    res_topic = []\n",
    "    for item in result1:  \n",
    "        res_topic+=str(item).split('\"')[1::2]\n",
    "    text = \" \".join(res_topic)\n",
    "    top_words = [item[0] for item in collections.Counter(text.split()).most_common(10)]\n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    # Display the generated image:\n",
    "    # the matplotlib way:\n",
    "    plt.figure(figsize=(100,100),dpi =300)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # take relative word frequencies into account, lower max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=80,stopwords=STOPWORDS,font_path = 'EraserRegular.ttf',\n",
    "                          background_color='blue',max_words=100,width=600,height=600).generate(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(lda_df_target.iloc[i:i+1]['name'].values[0])\n",
    "    plt.savefig(lda_df_target.iloc[i:i+1]['name'].values[0]+\".jpg\",figsize=(100,100),dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    #Sentimental Analysis by Random Foreast:\n",
    "    \n",
    "    temp_biz = lda_df_target.iloc[i:i+1]['business_id'].values[0]\n",
    "    temp_rev = review_raw[review_raw['business_id']==temp_biz]\n",
    "\n",
    "    traindf = temp_rev\n",
    "    traindf['sentiment'] = pd.Series([0]*len(traindf),index = traindf.index)\n",
    "    for i in traindf.index:\n",
    "        star = traindf.loc[i]['stars']\n",
    "        if star >= 4 or star == '5' or star == '4':\n",
    "            traindf.set_value(i, 'sentiment', 1)\n",
    "\n",
    "    traindf['review_text'] = pd.Series(traindf.text,index = traindf.index)\n",
    "\n",
    "    # Unigram Set:\n",
    "    start_time  = time.time()\n",
    "    fdist = nltk.FreqDist()\n",
    "    for i in traindf.index:\n",
    "        ly = traindf.text[i].lower()\n",
    "        ly = ly.decode('utf-8')\n",
    "        #1.Tokenization:\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(ly)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "        #2.Special Characters Removed:\n",
    "        tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "        tokens_lem_stem = []\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                #3.First Lemmatization, 4.then Stemming:\n",
    "                tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "        #5.Remove Punctuation:\n",
    "        word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "        #6.Remove stop words:\n",
    "        word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "        fdist = fdist.__add__(nltk.FreqDist(word_stop))\n",
    "        \n",
    "    fdist_uni = fdist\n",
    "    num_unigram = len(fdist)\n",
    "    # create features for training data\n",
    "    feat_uni = [i for [i,j] in fdist_uni.most_common(num_unigram)]\n",
    "    for unigram in feat_uni:\n",
    "        traindf[unigram] = pd.Series([0.0]*len(traindf),index = traindf.index)\n",
    "        \n",
    "    #Try to use unigram to analyze and obtain the feature:\n",
    "    fdist = nltk.FreqDist()\n",
    "    for i in traindf.index:\n",
    "        ly = traindf.review_text[i].lower()\n",
    "        ly = ly.decode('utf-8')\n",
    "        #1.Tokenization:\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(ly)\n",
    "        except UnicodeDecodeError:\n",
    "            print ly\n",
    "            continue\n",
    "        #2.Special Characters Removed:\n",
    "        tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "        tokens_lem_stem = []\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                #3.First Lemmatization, 4.then Stemming:\n",
    "                tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        #5.Remove Punctuation:\n",
    "        word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "        #6.Remove stop words:\n",
    "        word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "\n",
    "        #compute frequency distribution for all unigrams in one review:\n",
    "        fdist = nltk.FreqDist(word_stop)\n",
    "        for unigram in feat_uni:\n",
    "            traindf.set_value(i, unigram, fdist[unigram])\n",
    "            #if unigram in word_stop:\n",
    "            #    traindf.set_value(i, unigram, 1)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators= 100,criterion=\"entropy\",oob_score=True)#,min_samples_leaf=15)\n",
    "    clf = clf.fit(traindf[feat_uni],traindf['sentiment'])\n",
    "\n",
    "    print 'Correct rate of training set:',clf.score(traindf[feat_uni],traindf['sentiment'])\n",
    "    print 'Correct oob rate of training set:',clf.oob_score_\n",
    "\n",
    "\n",
    "    for word in top_words:\n",
    "        testdata = [0 if item != word else 1 for item in feat_uni]\n",
    "        print word,clf.predict(testdata)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
