{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from copy import copy, deepcopy\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from pandas.tseries.offsets import *\n",
    "import re\n",
    "import collections\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_ads = \"/Users/MaxTan/Documents/CU_16fall/BDA/Project/Data/\"\n",
    "\n",
    "user_raw = pd.read_csv(data_ads + \"wow3_user2.csv\", names = [\"user_id\", \"review_count\", \"average_stars\", \n",
    "                                               \"friends\", \"fans\", \"votes_cool\", \"votes_funny\", \n",
    "                                               \"votes_useful\", \"num\", \"degree\", \"coefficient\"])\n",
    "\n",
    "business_raw = pd.read_csv(data_ads+\"wow3_business_mysql.csv\", names = [\"business_id\", \"name\", \"latitude\", \n",
    "                                                           \"longitude\", \"stars\", \"categories\", \n",
    "                                                           \"review_count\", \"open\"])\n",
    "\n",
    "review_raw = pd.read_csv(data_ads+\"wow3_review_mysql.csv\", names = [\"review_id\", \"user_id\", \"business_id\", \n",
    "                                                       \"stars\", \"text\", \"date\", \"votes_funny\", \n",
    "                                                       \"votes_useful\"])\n",
    "\n",
    "all_raw = pd.read_csv(data_ads+\"wow3_all2.csv\", names = [\"business_id\", \n",
    "                                                       \"name\", \"stars_business\",\"business_review_count\",\n",
    "                                                       \"categories\", \"open\", \"review_id\", \n",
    "                                                       \"stars_review\", \"text\", \"date\", \"votes_funny\", \n",
    "                                                       \"votes_useful\",\"user_id\", \"user_review_count\", \n",
    "                                                       \"average_stars\",\"friends\", \"fans\", \"num\", \n",
    "                                                       \"degree\", \"coefficient\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.Topic Model and Sentimental Analysis by Using Review Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Sentimental Analysis (Random Forest Classification):\n",
    "review_pos = review_raw[(review_raw['stars']==5) | (review_raw['stars']=='5')]\n",
    "\n",
    "review_neg = review_raw[(review_raw['stars']<=2) | (review_raw['stars']=='1')|(review_raw['stars']=='2')]\n",
    "\n",
    "traindf = review_raw[(review_raw['stars']==5) | (review_raw['stars']=='5')|(review_raw['stars']<=2) | (review_raw['stars']=='1')|(review_raw['stars']=='2')]\n",
    "\n",
    "traindf['sentiment'] = pd.Series([0]*len(traindf),index = traindf.index)\n",
    "\n",
    "for i in traindf.index:\n",
    "    star = traindf.loc[i]['stars']\n",
    "    if star == 5 or star == '5':\n",
    "        traindf.set_value(i, 'sentiment', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_length, neg_length = len(traindf[traindf['sentiment']==1]),len(traindf[traindf['sentiment']==0])\n",
    "trainpos = traindf[traindf['sentiment']==1].iloc[:int(pos_length*0.8)]\n",
    "trainneg = traindf[traindf['sentiment']==0].iloc[:int(neg_length*0.8)]\n",
    "testpos = traindf[traindf['sentiment']==1].iloc[int(pos_length*0.8):]\n",
    "testneg = traindf[traindf['sentiment']==0].iloc[int(neg_length*0.8):]\n",
    "frames = [trainpos,trainneg]\n",
    "traindata = pd.concat(frames)\n",
    "frames = [testpos,testneg]\n",
    "testdata = pd.concat(frames)\n",
    "traindata['review_text'] = pd.Series(traindata.text,index = traindata.index)\n",
    "testdata['review_text'] = pd.Series(testdata.text,index = testdata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4973.09739804 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Unigram Set:\n",
    "start_time  = time.time()\n",
    "fdist = nltk.FreqDist()\n",
    "for i in traindata.index:\n",
    "    ly = traindata.text[i].lower()\n",
    "    ly = ly.decode('utf-8')\n",
    "    #1.Tokenization:\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove Stop Words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "    fdist = fdist.__add__(nltk.FreqDist(word_stop))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "fdist_uni = fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2969.48254895 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_unigram = 5000\n",
    "# create features for training data\n",
    "feat_uni = [i for [i,j] in fdist_uni.most_common(num_unigram)]\n",
    "for unigram in feat_uni:\n",
    "    traindata[unigram] = pd.Series([0.0]*len(traindata),index = traindata.index)\n",
    "#create bigram features for training set:\n",
    "\n",
    "#Try to use unigram to analyze and obtain the feature:\n",
    "fdist = nltk.FreqDist()\n",
    "for i in traindata.index:\n",
    "    ly = traindata.review_text[i].lower()\n",
    "    \n",
    "    ly = ly.decode('utf-8')\n",
    "    #1.Tokenization:\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "    except UnicodeDecodeError:\n",
    "        print ly\n",
    "        continue\n",
    "        \n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    \n",
    "    #compute frequency distribution for all the unigram in one song:\n",
    "    fdist = nltk.FreqDist(word_stop)\n",
    "    for unigram in feat_uni:\n",
    "        traindata.set_value(i, unigram, fdist[unigram])\n",
    "        #if unigram in word_stop:\n",
    "        #    traindata.set_value(i, unigram, 1)\n",
    "\n",
    "\n",
    "\n",
    "#change features for test data\n",
    "for unigram in feat_uni:\n",
    "    testdata[unigram] = pd.Series([0.0]*len(testdata),index = testdata.index)\n",
    "\n",
    "\n",
    "#Try to use unigram to analyze and obtain the feature:\n",
    "fdist = nltk.FreqDist()\n",
    "for i in testdata.index:\n",
    "    ly = testdata.review_text[i].lower()\n",
    "    ly = ly.decode('utf-8')\n",
    "    #1.Tokenization:\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "    except UnicodeDecodeError:\n",
    "        print ly\n",
    "        break\n",
    "        \n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]    \n",
    "    \n",
    "    \n",
    "    #compute frequency distribution for all the unigram in one song:\n",
    "    fdist = nltk.FreqDist(word_stop)\n",
    "    for unigram in feat_uni:\n",
    "        testdata.set_value(i, unigram, fdist[unigram])\n",
    "        #if unigram in word_stop:\n",
    "        #    testdata.set_value(i, unigram, 1)    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct oob rate of training set: 0.923025765344\n",
      "Correct rate of test set: 0.923895323895\n",
      "--- 1425.03806591 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 500,criterion=\"entropy\",oob_score=True)#,min_samples_leaf=15)\n",
    "clf = clf.fit(traindata[feat_uni],traindata['sentiment'])\n",
    "\n",
    "print 'Correct oob rate of training set:',clf.oob_score_\n",
    "print 'Correct rate of test set:',clf.score(testdata[feat_uni],testdata['sentiment'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "res = clf.predict(testdata[feat_uni])\n",
    "for i in set(testdata.sentiment.values):\n",
    "    print 'Results of test set with '+str(i)+' decades difference :',sum(map(abs,res - testdata['sentiment'])==i)/float(len(res))\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct oob rate of training set: 0.922618153734\n",
      "Correct rate of test set: 0.922865722866\n",
      "--- 1407.38278103 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 500,criterion=\"entropy\",oob_score=True)#,min_samples_leaf=15)\n",
    "clf = clf.fit(traindata[feat_uni],traindata['sentiment'])\n",
    "\n",
    "#print 'Correct rate of training set:',clf.score(traindata[feat_uni],traindata['sentiment'])\n",
    "print 'Correct oob rate of training set:',clf.oob_score_\n",
    "print 'Correct rate of test set:',clf.score(testdata[feat_uni],testdata['sentiment'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "res = clf.predict(testdata[feat_uni])\n",
    "for i in set(testdata.sentiment.values):\n",
    "    print 'Results of test set with '+str(i)+' decades difference :',sum(map(abs,res - testdata['sentiment'])==i)/float(len(res))\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate of train set: 0.852702035913\n",
      "Correct rate of test set: 0.827456027456\n"
     ]
    }
   ],
   "source": [
    "#Methods 2: Naive Bayes:\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(traindata[feat_uni],traindata['sentiment']).predict(traindata[feat_uni])\n",
    "print \"Correct rate of train set:\", float((traindata['sentiment'].values == y_pred).sum())/len(traindata)\n",
    "\n",
    "y_pred = gnb.fit(traindata[feat_uni],traindata['sentiment']).predict(testdata[feat_uni])\n",
    "print \"Correct rate of test set:\", float((testdata['sentiment'].values == y_pred).sum())/len(testdata)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
