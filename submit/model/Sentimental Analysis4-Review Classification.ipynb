{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from copy import copy, deepcopy\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from pandas.tseries.offsets import *\n",
    "import re\n",
    "import collections\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_ads = \"/Users/MaxTan/Documents/CU_16fall/BDA/Project/Data/\"\n",
    "\n",
    "user_raw = pd.read_csv(data_ads + \"wow3_user2.csv\", names = [\"user_id\", \"review_count\", \"average_stars\", \n",
    "                                               \"friends\", \"fans\", \"votes_cool\", \"votes_funny\", \n",
    "                                               \"votes_useful\", \"num\", \"degree\", \"coefficient\"])\n",
    "\n",
    "business_raw = pd.read_csv(data_ads+\"wow3_business_mysql.csv\", names = [\"business_id\", \"name\", \"latitude\", \n",
    "                                                           \"longitude\", \"stars\", \"categories\", \n",
    "                                                           \"review_count\", \"open\"])\n",
    "\n",
    "review_raw = pd.read_csv(data_ads+\"wow3_review_mysql.csv\", names = [\"review_id\", \"user_id\", \"business_id\", \n",
    "                                                       \"stars\", \"text\", \"date\", \"votes_funny\", \n",
    "                                                       \"votes_useful\"])\n",
    "\n",
    "all_raw = pd.read_csv(data_ads+\"wow3_all2.csv\", names = [\"business_id\", \n",
    "                                                       \"name\", \"stars_business\",\"business_review_count\",\n",
    "                                                       \"categories\", \"open\", \"review_id\", \n",
    "                                                       \"stars_review\", \"text\", \"date\", \"votes_funny\", \n",
    "                                                       \"votes_useful\",\"user_id\", \"user_review_count\", \n",
    "                                                       \"average_stars\",\"friends\", \"fans\", \"num\", \n",
    "                                                       \"degree\", \"coefficient\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.Topic Model and Sentimental Analysis by Using Review Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39691, 8)\n",
      "(18577, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Sentimental Analysis (Random Forest Classification):\n",
    "review_pos = review_raw[(review_raw['stars']==5) | (review_raw['stars']=='5')]\n",
    "\n",
    "review_neg = review_raw[(review_raw['stars']<=2) | (review_raw['stars']=='1')|(review_raw['stars']=='2')]\n",
    "\n",
    "traindf = review_raw[(review_raw['stars']==5) | (review_raw['stars']=='5')|(review_raw['stars']<=2) | (review_raw['stars']=='1')|(review_raw['stars']=='2')]\n",
    "\n",
    "traindf['sentiment'] = pd.Series([0]*len(traindf),index = traindf.index)\n",
    "\n",
    "for i in traindf.index:\n",
    "    star = traindf.loc[i]['stars']\n",
    "    if star == 5 or star == '5':\n",
    "        traindf.set_value(i, 'sentiment', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39691 18577\n"
     ]
    }
   ],
   "source": [
    "pos_length, neg_length = len(traindf[traindf['sentiment']==1]),len(traindf[traindf['sentiment']==0])\n",
    "trainpos = traindf[traindf['sentiment']==1].iloc[:int(pos_length*0.8)]\n",
    "trainneg = traindf[traindf['sentiment']==0].iloc[:int(neg_length*0.8)]\n",
    "testpos = traindf[traindf['sentiment']==1].iloc[int(pos_length*0.8):]\n",
    "testneg = traindf[traindf['sentiment']==0].iloc[int(neg_length*0.8):]\n",
    "frames = [trainpos,trainneg]\n",
    "traindata = pd.concat(frames)\n",
    "frames = [testpos,testneg]\n",
    "testdata = pd.concat(frames)\n",
    "traindata['review_text'] = pd.Series(traindata.text,index = traindata.index)\n",
    "testdata['review_text'] = pd.Series(testdata.text,index = testdata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7787.71427894 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Unigram Set:\n",
    "start_time  = time.time()\n",
    "fdist = nltk.FreqDist()\n",
    "for i in traindata.index:\n",
    "    ly = traindata.text[i].lower()\n",
    "    ly = ly.decode('utf-8')\n",
    "    #1.Tokenization:\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove Stop Words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "    fdist = fdist.__add__(nltk.FreqDist(word_stop))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "fdist_uni = fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2299.18544507 seconds ---\n",
      "--- 575.722703934 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_unigram = 5000\n",
    "# create features for training data\n",
    "feat_uni = [i for [i,j] in fdist_uni.most_common(num_unigram)]\n",
    "for unigram in feat_uni:\n",
    "    traindata[unigram] = pd.Series([0.0]*len(traindata),index = traindata.index)\n",
    "#create bigram features for training set:\n",
    "\n",
    "#Try to use unigram to analyze and obtain the feature:\n",
    "fdist = nltk.FreqDist()\n",
    "for i in traindata.index:\n",
    "    ly = traindata.review_text[i].lower()\n",
    "    \n",
    "    ly = ly.decode('utf-8')\n",
    "    #1.Tokenization:\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "    except UnicodeDecodeError:\n",
    "        print ly\n",
    "        continue\n",
    "        \n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    \n",
    "    #compute frequency distribution for all the unigram in one song:\n",
    "    fdist = nltk.FreqDist(word_stop)\n",
    "    for unigram in feat_uni:\n",
    "        traindata.set_value(i, unigram, fdist[unigram])\n",
    "        #if unigram in word_stop:\n",
    "        #    traindata.set_value(i, unigram, 1)\n",
    "\n",
    "\n",
    "\n",
    "#change features for test data\n",
    "for unigram in feat_uni:\n",
    "    testdata[unigram] = pd.Series([0.0]*len(testdata),index = testdata.index)\n",
    "\n",
    "\n",
    "#Try to use unigram to analyze and obtain the feature:\n",
    "fdist = nltk.FreqDist()\n",
    "for i in testdata.index:\n",
    "    ly = testdata.review_text[i].lower()\n",
    "    ly = ly.decode('utf-8')\n",
    "    #1.Tokenization:\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "    except UnicodeDecodeError:\n",
    "        print ly\n",
    "        break\n",
    "        \n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]    \n",
    "    \n",
    "    \n",
    "    #compute frequency distribution for all the unigram in one song:\n",
    "    fdist = nltk.FreqDist(word_stop)\n",
    "    for unigram in feat_uni:\n",
    "        testdata.set_value(i, unigram, fdist[unigram])\n",
    "        #if unigram in word_stop:\n",
    "        #    testdata.set_value(i, unigram, 1)    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct oob rate of training set: 0.923025765344\n",
      "Correct rate of test set: 0.925096525097\n",
      "--- 1621.26505613 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 500,criterion=\"entropy\",oob_score=True)#,min_samples_leaf=15)\n",
    "clf = clf.fit(traindata[feat_uni],traindata['sentiment'])\n",
    "\n",
    "print 'Correct oob rate of training set:',clf.oob_score_\n",
    "print 'Correct rate of test set:',clf.score(testdata[feat_uni],testdata['sentiment'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "res = clf.predict(testdata[feat_uni])\n",
    "for i in set(testdata.sentiment.values):\n",
    "    print 'Results of test set with '+str(i)+' decades difference :',sum(map(abs,res - testdata['sentiment'])==i)/float(len(res))\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate of training set: 0.999957093515\n",
      "Correct oob rate of training set: 0.922725419947\n",
      "Correct rate of test set: 0.922951522952\n",
      "Results of test set with 0 decades difference : 0.922951522952\n",
      "Results of test set with 1 decades difference : 0.0770484770485\n",
      "--- 1464.67448497 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 500,criterion=\"entropy\",oob_score=True)#,min_samples_leaf=15)\n",
    "clf = clf.fit(traindata[feat_uni],traindata['sentiment'])\n",
    "\n",
    "#print 'Correct rate of training set:',clf.score(traindata[feat_uni],traindata['sentiment'])\n",
    "print 'Correct oob rate of training set:',clf.oob_score_\n",
    "print 'Correct rate of test set:',clf.score(testdata[feat_uni],testdata['sentiment'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "res = clf.predict(testdata[feat_uni])\n",
    "for i in set(testdata.sentiment.values):\n",
    "    print 'Results of test set with '+str(i)+' decades difference :',sum(map(abs,res - testdata['sentiment'])==i)/float(len(res))\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate of test set: 0.827456027456\n",
      "--- 16.7695269585 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Methods 2: Naive Bayes:\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(traindata[feat_uni],traindata['sentiment']).predict(testdata[feat_uni])\n",
    "print \"Correct rate of test set:\", float((testdata['sentiment'].values == y_pred).sum())/len(testdata)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
