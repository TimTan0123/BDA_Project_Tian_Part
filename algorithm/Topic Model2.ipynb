{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from copy import copy, deepcopy\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from pandas.tseries.offsets import *\n",
    "import re\n",
    "import collections\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ads = \"/Users/MaxTan/Documents/CU_16fall/BDA/Project/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_raw = pd.read_csv(data_ads + \"wow3_user2.csv\", names = [\"user_id\", \"review_count\", \"average_stars\", \n",
    "                                               \"friends\", \"fans\", \"votes_cool\", \"votes_funny\", \n",
    "                                               \"votes_useful\", \"num\", \"degree\", \"coefficient\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_raw = pd.read_csv(data_ads+\"wow3_business_mysql.csv\", names = [\"business_id\", \"name\", \"latitude\", \n",
    "                                                           \"longitude\", \"stars\", \"categories\", \n",
    "                                                           \"review_count\", \"open\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (3,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "review_raw = pd.read_csv(data_ads+\"wow3_review_mysql.csv\", names = [\"review_id\", \"user_id\", \"business_id\", \n",
    "                                                       \"stars\", \"text\", \"date\", \"votes_funny\", \n",
    "                                                       \"votes_useful\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_raw = pd.read_csv(data_ads+\"wow3_all2.csv\", names = [\"business_id\", \n",
    "                                                       \"name\", \"stars_business\",\"business_review_count\",\n",
    "                                                       \"categories\", \"open\", \"review_id\", \n",
    "                                                       \"stars_review\", \"text\", \"date\", \"votes_funny\", \n",
    "                                                       \"votes_useful\",\"user_id\", \"user_review_count\", \n",
    "                                                       \"average_stars\",\"friends\", \"fans\", \"num\", \n",
    "                                                       \"degree\", \"coefficient\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.Topic Model by Key Word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2210.07916307 seconds ---\n",
      "3626\n"
     ]
    }
   ],
   "source": [
    "#The following codes take 20 minutes to generate a new csv file, which I've store it in the same folder:\n",
    "\n",
    "bus_gb_df = deepcopy(all_raw)\n",
    "#Group by different businesses for different rates:\n",
    "bus_df = bus_gb_df.groupby(['business_id']).mean()\n",
    "bus_df['business_id'] = bus_df.index\n",
    "bus_df.index = range(len(bus_df))\n",
    "bus = list(set(bus_df['business_id']))\n",
    "ind = [True if i in bus else False for i in business_raw['business_id']]\n",
    "business_revised = business_raw[ind]\n",
    "\n",
    "#Merge two tables:\n",
    "lda_df = pd.merge(bus_df,business_revised,on='business_id')\n",
    "\n",
    "start_time = time.time()\n",
    "bus_topic = dict()\n",
    "\n",
    "for i in range(len(all_raw)):\n",
    "    ly = all_raw.text[i].lower()\n",
    "    ly = ly.decode('utf-8')\n",
    "    \n",
    "    #1.Tokenization:\n",
    "    tokens = nltk.word_tokenize(ly)\n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    tokens = word_stop\n",
    "    \n",
    "    \n",
    "    temp_bus = all_raw.business_id[i]\n",
    "    if temp_bus not in bus_topic:\n",
    "        bus_topic[temp_bus] = tokens\n",
    "    else:\n",
    "        bus_topic[temp_bus] += tokens\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "lda_df['tokens'] = pd.Series(['']*len(lda_df),index = lda_df.index)\n",
    "print len(bus_topic)\n",
    "for bus in bus_topic:\n",
    "    #print bus\n",
    "    tempdf = lda_df[lda_df['business_id']==bus]\n",
    "    temptext = tempdf['categories'].values[0].lower()[1:-1]+', '.join(bus_topic[bus])\n",
    "    lda_df.set_value([tempdf.index[0]],'tokens',temptext)\n",
    "\n",
    "lda_df.to_csv('business_LDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'bars, nightlifeorder, chicken, wing, look, veri, unappet, especi, pick, one, feather, come, understand, thing, happen, situat, wa, handl, veri, unprofession, sure, server, wa, embarrass, seem, though, avoid, issu, wa, brought, attent, typic, tavern, basic, bar, menu, burger, sandwich, fri, munchi, deep, fri, jalapeno, pretzel, great, chang, usual, fri, chees, stick, usual, order, fri, fish, sandwich, great, size, fill, servic, okay, casual, prompt, main, dine, area, away, bar, nice, quiet, full, pittsburgh, sport, memorabilia, bad, place, look, around, wait, order, love, witti, sign, put, get, think, drive, rt, 88, seem, never, end, construct, zone'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_df.tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_df2 = pd.read_csv('business_LDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bars, nightlifeorder, chicken, wing, look, veri, unappet, especi, pick, one, feather, come, understand, thing, happen, situat, wa, handl, veri, unprofession, sure, server, wa, embarrass, seem, though, avoid, issu, wa, brought, attent, typic, tavern, basic, bar, menu, burger, sandwich, fri, munchi, deep, fri, jalapeno, pretzel, great, chang, usual, fri, chees, stick, usual, order, fri, fish, sandwich, great, size, fill, servic, okay, casual, prompt, main, dine, area, away, bar, nice, quiet, full, pittsburgh, sport, memorabilia, bad, place, look, around, wait, order, love, witti, sign, put, get, think, drive, rt, 88, seem, never, end, construct, zone'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_df2.tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_word = 'pizza'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:26: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "lda_df2 = pd.read_csv('business_LDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. Sort by stars:\n",
    "ind = [True if search_word in token else False for token in lda_df2.tokens.values]\n",
    "lda_df_sub = lda_df2[ind]\n",
    "lda_df_target = lda_df_sub.sort(['stars_review'],ascending=[0]).iloc[:6]\n",
    "doc_set = list(lda_df_target.tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#2. Sort by number of key word:\n",
    "lda_df2['count_sw'] = pd.Series([0]*len(lda_df2),index = lda_df2.index)\n",
    "for i in range(len(lda_df2)):\n",
    "    temp_value = collections.Counter([item.strip() for item in lda_df2.tokens[i].split(\",\")])[search_word]\n",
    "    lda_df2.set_value(i,'count_sw',temp_value)\n",
    "lda_df_target = lda_df2.sort(['count_sw'],ascending=[0]).iloc[:10]\n",
    "doc_set = list(lda_df_target.tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'good', 'wa', 'beer', 'thi', 'nt', 'pizza', 'place', 'veri', 'order']\n",
      "['wa', 'mineo', 'chee', 'place', 'thi', 'nt', 'pizza', 'good', 'pittsburgh', 'like']\n",
      "['wa', 'gelato', 'place', 'thi', 'flavor', 'nt', 'pizza', 'good', 'like', 'mercurio']\n",
      "['wa', 'chee', 'thi', 'nt', 'best', 'pizza', 'sauc', 'place', 'fiori', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wa', 'thi', 'aiello', 'pizza', 'good', 'place', 'nt', 'chee', 'mineo', 'best']\n",
      "['good', 'wa', 'food', 'beer', 'place', 'thi', 'church', 'nt', 'great', 'brew']\n",
      "['good', 'wa', 'place', 'thi', 'nt', 'pizza', 'dinett', 'wine', 'one', 'like']\n",
      "['good', 'wa', 'nt', 'pizza', 'great', 'porch', 'thi', 'food', 'place', 'order']\n",
      "['good', 'wa', 'thi', 'pasta', 'pizza', 'wait', 'food', 'nt', 'restaur', 'place']\n",
      "['wa', 'get', 'beto', 'chee', 'thi', 'nt', 'pizza', 'top', 'like', 'cold']\n",
      "--- 110.998905897 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(doc_set)):\n",
    "    texts = []\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = doc_set[i].lower()\n",
    "    \n",
    "    #1.Tokenization:\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    \n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    \n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, #4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    \n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")] \n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(word_stop)\n",
    "\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=50)\n",
    "    \n",
    "    result = ldamodel.print_topics(num_topics=10, num_words=10)\n",
    "    \n",
    "    result1 = [j[1] for j in result]\n",
    "    \n",
    "    res_topic = []\n",
    "    \n",
    "    for item in result1:\n",
    "        \n",
    "        res_topic+=str(item).split('\"')[1::2]\n",
    "    \n",
    "    text = \" \".join(res_topic)\n",
    "    \n",
    "    top_words = [item[0] for item in collections.Counter(text.split()).most_common(10)]\n",
    "    print top_words\n",
    "    \n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "\n",
    "    # Display the generated image:\n",
    "    # the matplotlib way:\n",
    "\n",
    "    plt.figure(figsize=(100,100),dpi =300)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # take relative word frequencies into account, lower max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=80,stopwords=STOPWORDS,font_path = 'EraserRegular.ttf',\n",
    "                          background_color='blue',max_words=100,width=600,height=600).generate(text)\n",
    "                          #relative_scaling=.8).generate(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(lda_df_target.iloc[i:i+1]['name'].values[0])\n",
    "    plt.savefig(lda_df_target.iloc[i:i+1]['name'].values[0]+\".jpg\",figsize=(100,100),dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sentimental Analysis(Other Popular Words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_biz = lda_df_target.iloc[i:i+1]['business_id'].values[0]\n",
    "temp_rev = review_raw[review_raw['business_id']==temp_biz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1', '2', '3', '4', '5'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(temp_rev.stars.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_rev_pos = temp_rev[(temp_rev['stars']==5) | (temp_rev['stars']=='5')]\n",
    "\n",
    "temp_rev_neg = temp_rev[(temp_rev['stars']<=2) | (temp_rev['stars']=='1')|(temp_rev['stars']=='2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 8) (27, 8)\n"
     ]
    }
   ],
   "source": [
    "print temp_rev_pos.shape,temp_rev_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are only so many ways to make the American bastardized version of a pizza, but for some reason there are as many pizza shops as there are stars in a clear night sky. As they may be stunning to behold, from afar each one is pretty much the same thing. This shop is another place that tosses sauce and cheese on bread and serves it to their patrons. Ok, so they DO have a slightly different take, throwing the cold shredded cheese onto the pie as soon as the dough is out of the oven. This does not melt the cheese, and I do see how the pizza would be cold after heaping on the cheese.  Some customers may like it, me, not so much. I tried the Italian hoagie.  It was a good size and the meat was flavorful. But that's about it. They put oil on the sandwich, but no other dressing. I expected a nice tangy flavor to compliment the meat but was disappointed. My date ordered a hot sausage hoagie. I didn't sample it, he liked it, but it was way too messy. The bread got soggy and the darn thing fell apart. It looked more like tomato soup with sausage than a sandwich.  The staff did seem pretty nice, even trying to keep a smile while dealing with the stress of a busy crowded restaurant. The dining room is also a bit cramped. There are plenty of booths, but the tables are rather close together and hard to maneuver between. Hungry patrons also are constantly getting up to visit the soda fountain. When they are busy, it is a claustrophobic nightmare.\""
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_rev_neg.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39691, 8)\n",
      "(18577, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MaxTan/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Sentimental Analysis(Random Forest):\n",
    "review_pos = review_raw[(review_raw['stars']==5) | (review_raw['stars']=='5')]\n",
    "\n",
    "review_neg = review_raw[(review_raw['stars']<=2) | (review_raw['stars']=='1')|(review_raw['stars']=='2')]\n",
    "\n",
    "print review_pos.shape\n",
    "print review_neg.shape\n",
    "\n",
    "traindf = review_raw[(review_raw['stars']==5) | (review_raw['stars']=='5')|(review_raw['stars']<=2) | (review_raw['stars']=='1')|(review_raw['stars']=='2')]\n",
    "\n",
    "traindf['sentiment'] = pd.Series([0]*len(traindf),index = traindf.index)\n",
    "\n",
    "for i in traindf.index:\n",
    "    star = traindf.loc[i]['stars']\n",
    "    if star == 5 or star == '5':\n",
    "        traindf.set_value(i, 'sentiment', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31820\n",
      "--- 386.350394011 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#remove those songs that can't be tokenized(have unicode decode error):\n",
    "start_time  = time.time()\n",
    "\n",
    "newdf = pd.DataFrame()\n",
    "for i in traindf.index:\n",
    "    ly = traindf.loc[i]['text'].lower()\n",
    "    #ly = ly.translate(None, string.punctuation)\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "        tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "        tempdf = traindf[i:i+1]\n",
    "        frames = [newdf,tempdf]\n",
    "        newdf = pd.concat(frames) \n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "print len(newdf)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39691 18577\n"
     ]
    }
   ],
   "source": [
    "pos_length, neg_length = len(traindf[traindf['sentiment']==1]),len(traindf[traindf['sentiment']==0])\n",
    "print pos_length, neg_length\n",
    "trainpos = traindf[traindf['sentiment']==1].iloc[:int(pos_length*0.8)]\n",
    "trainneg = traindf[traindf['sentiment']==0].iloc[:int(neg_length*0.8)]\n",
    "testpos = traindf[traindf['sentiment']==1].iloc[int(pos_length*0.8):]\n",
    "testneg = traindf[traindf['sentiment']==0].iloc[int(neg_length*0.8):]\n",
    "frames = [trainpos,trainneg]\n",
    "traindata = pd.concat(frames)\n",
    "frames = [testpos,testneg]\n",
    "testdata = pd.concat(frames)\n",
    "traindata['review_text'] = pd.Series(traindata.text,index = traindata.index)\n",
    "testdata['review_text'] = pd.Series(testdata.text,index = testdata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7787.71427894 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Unigram Set:\n",
    "start_time  = time.time()\n",
    "fdist = nltk.FreqDist()\n",
    "for i in traindata.index:\n",
    "    ly = traindata.text[i].lower()\n",
    "    ly = ly.decode('utf-8')\n",
    "    #1.Tokenization:\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "        \n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "\n",
    "    fdist = fdist.__add__(nltk.FreqDist(word_stop))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "fdist_uni = fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ly = ly.decode('utf-8')\n",
    "#1.Tokenization:\n",
    "tokens = nltk.word_tokenize(ly)\n",
    "#2.Special Characters Removed:\n",
    "tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "tokens_lem_stem = []\n",
    "for word in tokens:\n",
    "    try:\n",
    "        #3.First Lemmatization, 4.then Stemming:\n",
    "        tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "\n",
    "#5.Remove Punctuation:\n",
    "word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "#6.Remove stop words:\n",
    "word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "\n",
    "final_list = word_stop\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2299.18544507 seconds ---\n",
      "--- 575.722703934 seconds ---\n"
     ]
    }
   ],
   "source": [
    "num_unigram = 5000\n",
    "# create features for training data\n",
    "feat_uni = [i for [i,j] in fdist_uni.most_common(num_unigram)]\n",
    "for unigram in feat_uni:\n",
    "    traindata[unigram] = pd.Series([0.0]*len(traindata),index = traindata.index)\n",
    "#create bigram features for training set:\n",
    "start_time = time.time()\n",
    "#Try to use unigram to analyze and obtain the feature:\n",
    "fdist = nltk.FreqDist()\n",
    "for i in traindata.index:\n",
    "    ly = traindata.review_text[i].lower()\n",
    "    \n",
    "    ly = ly.decode('utf-8')\n",
    "    #1.Tokenization:\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "    except UnicodeDecodeError:\n",
    "        print ly\n",
    "        continue\n",
    "        \n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    \n",
    "    #compute frequency distribution for all the unigram in one song:\n",
    "    fdist = nltk.FreqDist(word_stop)\n",
    "    for unigram in feat_uni:\n",
    "        traindata.set_value(i, unigram, fdist[unigram])\n",
    "        #if unigram in word_stop:\n",
    "        #    traindata.set_value(i, unigram, 1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#change features for test data\n",
    "for unigram in feat_uni:\n",
    "    testdata[unigram] = pd.Series([0.0]*len(testdata),index = testdata.index)\n",
    "\n",
    "start_time = time.time()\n",
    "#Try to use unigram to analyze and obtain the feature:\n",
    "fdist = nltk.FreqDist()\n",
    "for i in testdata.index:\n",
    "    ly = testdata.review_text[i].lower()\n",
    "    ly = ly.decode('utf-8')\n",
    "    #1.Tokenization:\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(ly)\n",
    "    except UnicodeDecodeError:\n",
    "        print ly\n",
    "        break\n",
    "        \n",
    "    #2.Special Characters Removed:\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', mystring) for mystring in tokens]\n",
    "    tokens_lem_stem = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            #3.First Lemmatization, 4.then Stemming:\n",
    "            tokens_lem_stem += [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word))]\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    #5.Remove Punctuation:\n",
    "    word_punct = [w.lower() for w in tokens_lem_stem if w not in string.punctuation]\n",
    "    #6.Remove stop words:\n",
    "    word_stop = [w for w in word_punct if w not in stopwords.words(\"english\")]    \n",
    "    \n",
    "    \n",
    "    #compute frequency distribution for all the unigram in one song:\n",
    "    fdist = nltk.FreqDist(word_stop)\n",
    "    for unigram in feat_uni:\n",
    "        testdata.set_value(i, unigram, fdist[unigram])\n",
    "        #if unigram in word_stop:\n",
    "        #    testdata.set_value(i, unigram, 1)    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate of training set: 0.999957093515\n",
      "Correct oob rate of training set: 0.922725419947\n",
      "Correct rate of test set: 0.922951522952\n",
      "Results of test set with 0 decades difference : 0.922951522952\n",
      "Results of test set with 1 decades difference : 0.0770484770485\n",
      "--- 1464.67448497 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 500,criterion=\"entropy\",oob_score=True)#,min_samples_leaf=15)\n",
    "clf = clf.fit(traindata[feat_uni],traindata['sentiment'])\n",
    "\n",
    "print 'Correct rate of training set:',clf.score(traindata[feat_uni],traindata['sentiment'])\n",
    "print 'Correct oob rate of training set:',clf.oob_score_\n",
    "print 'Correct rate of test set:',clf.score(testdata[feat_uni],testdata['sentiment'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "res = clf.predict(testdata[feat_uni])\n",
    "for i in set(testdata.sentiment.values):\n",
    "    print 'Results of test set with '+str(i)+' decades difference :',sum(map(abs,res - testdata['sentiment'])==i)/float(len(res))\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate of training set: 0.823997597237\n",
      "Correct oob rate of training set: 0.819599682492\n",
      "Correct rate of test set: 0.823852423852\n",
      "Results of test set with 0 decades difference : 0.823852423852\n",
      "Results of test set with 1 decades difference : 0.176147576148\n",
      "--- 151.296142101 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 100,criterion=\"entropy\",oob_score=True,min_samples_leaf=100)\n",
    "clf = clf.fit(traindata[feat_uni],traindata['sentiment'])\n",
    "\n",
    "print 'Correct rate of training set:',clf.score(traindata[feat_uni],traindata['sentiment'])\n",
    "print 'Correct oob rate of training set:',clf.oob_score_\n",
    "print 'Correct rate of test set:',clf.score(testdata[feat_uni],testdata['sentiment'])\n",
    "\n",
    "\"\"\"\n",
    "res = clf.predict(testdata[feat_uni])\n",
    "for i in set(testdata.sentiment.values):\n",
    "    print 'Results of test set with '+str(i)+' decades difference :',sum(map(abs,res - testdata['sentiment'])==i)/float(len(res))\n",
    "\n",
    "\"\"\"\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate of training set: 0.905498466093\n",
      "Correct oob rate of training set: 0.891940016734\n",
      "Correct rate of test set: 0.893264693265\n",
      "Results of test set with 0 decades difference : 0.893264693265\n",
      "Results of test set with 1 decades difference : 0.106735306735\n",
      "--- 863.067273855 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 500,criterion=\"entropy\",oob_score=True,min_samples_leaf=15)\n",
    "clf = clf.fit(traindata[feat_uni],traindata['sentiment'])\n",
    "\n",
    "print 'Correct rate of training set:',clf.score(traindata[feat_uni],traindata['sentiment'])\n",
    "print 'Correct oob rate of training set:',clf.oob_score_\n",
    "print 'Correct rate of test set:',clf.score(testdata[feat_uni],testdata['sentiment'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "res = clf.predict(testdata[feat_uni])\n",
    "for i in set(testdata.sentiment.values):\n",
    "    print 'Results of test set with '+str(i)+' decades difference :',sum(map(abs,res - testdata['sentiment'])==i)/float(len(res))\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Methods 2: Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate of test set: 0.172543972544\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(traindata[feat_uni],traindata['sentiment']).predict(testdata[feat_uni])\n",
    "\n",
    "print \"Correct rate of test set:\", float((testdata['sentiment'].values == y_pred).sum())/len(testdata)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Methods 3: SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf = svm.SVR()\n",
    "clf.fit(traindata[feat_uni],traindata['sentiment']) \n",
    "clf.predict(testdata[feat_uni])\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
